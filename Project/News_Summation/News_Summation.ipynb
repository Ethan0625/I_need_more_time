{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News summation\n",
    "### 뉴스 요약봇 만들기\n",
    "***\n",
    "#### Step 1. 데이터 수집하기\n",
    "\n",
    "    1. 뉴스 기사 데이터 가져오기  \n",
    "\n",
    "#### Step 2. 데이터 전처리하기 (추상적 요약) \n",
    "\n",
    "    -실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 \n",
    "     추가 사용하여 텍스트를 정규화 또는 정제해 보세요.  \n",
    "     \n",
    "    -만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 \n",
    "     불용어를 제거하는 것이 좋을지 고민해보세요.  \n",
    "\n",
    "#### Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
    "    일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 \n",
    "    더 나은 성능을 얻을 수 있어요.  \n",
    "    \n",
    "    실습 내용을 참고하여 어텐션 메커니즘을 사용한 seq2seq를 설계해 보세요.\n",
    "\n",
    "#### Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "    원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해보세요.  \n",
    "    \n",
    "#### Step 5. Summa을 이용해서 추출적 요약해보기\n",
    "***\n",
    "#### 평가문항  \n",
    "**1. Abstractive 모델 구성을 위한 텍스트 전처리 단계가 체계적으로 진행되었다.**  \n",
    "-분석단계, 정제단계, 정규화와 불용어 제거, 데이터셋 분리, 인코딩 과정이 빠짐없이 체계적으로 진행되었다.\n",
    "  \n",
    "**2. 텍스트 요약모델이 성공적으로 학습되었음을 확인하였다.**  \n",
    "-모델학습이 안정적으로 수렴되었음을 그래프를 통해 확인하였으며, 실제 요약문과 유사한 요약문장을 얻을 수 있었다.\n",
    "  \n",
    "**3. Extractive 요약을 시도해 보고 Abstractive 요약 결과과 함께 비교해 보았다.**  \n",
    "-두 요약 결과를 문법완성도 측면과 핵심단어 포함 측면으로 나누어 비교분석 결과를 제시하였다.\n",
    "***\n",
    "### Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "import urllib.request\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 뉴스 기사 데이터 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노드를 진행하면서 다운받은 뉴스 요약 data 가져오기\n",
    "# https://github.com/sunnysai12345/News_Summary\n",
    "data_dir = os.getenv('HOME')+'/aiffel/news_summarization/data'\n",
    "data_path = join(data_dir, 'news_summary_more.csv')\n",
    "data = pd.read_csv(data_path, encoding='iso-8859-1')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기 (추상적 요약)\n",
    "    -실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 \n",
    "     추가 사용하여 텍스트를 정규화 또는 정제해 보세요.  \n",
    "    -만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 \n",
    "     불용어를 제거하는 것이 좋을지 고민해보세요.  \n",
    "***\n",
    "**1. 중복 샘플 제거**   \n",
    "\n",
    "중복 샘플을 제거하기전 전체 샘플의 수를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :',len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 칼럼당 98401개의 데이터가 있는것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines열의 unique : 98280\n",
      "text열의 unique : 98360\n"
     ]
    }
   ],
   "source": [
    "for c in data.columns:\n",
    "    print('{}열의 unique : {}'.format(c, data[c].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유니크한 데이터의 갯수를 확인해보니 두개의 칼럼 모두 전체 샘플수와 차이가 나는것을 확인할 수 있습니다.  \n",
    "이러한 중복 샘플을 제거해 주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns:\n",
    "    data.drop_duplicates(subset = [c], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터프레임의 drop_duplicates()를 사용하여 중복 샘플을 지워줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98262\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :',len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 샘플수와 비교해보았을 때 중복 샘플이 제거 된 것을 확인할 수 있습니다.\n",
    "***\n",
    "**2. 결측 샘플 제거**  \n",
    "\n",
    "Null값을 가지는 데이터 샘플을 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".isnull().sum()로 확인해본 결과 다행히 결측치는 없는 것으로 확인되었습니다.  \n",
    "참고로 Null값 제거에는 dropna() 함수를 사용하면 됩니다.\n",
    "***   \n",
    "**3. 텍스트 정규화 및 불용어 제거**  \n",
    "\n",
    "i'am, it'll 등 줄여진 표현들을 정리하여 연산량을 줄이도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드에서 사용하였던 정규화 사전을 이용하도록 하겠습니다.\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 소문자화 => 섞여있을 수 있는 ltml태그 제거 => 특수문자 제거 => 정규화 => 불용어 제거\n",
    "# 위의 과정을 위한 함수 생성\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 함수를 활용하여 text칼럼과 headline칼럼을 정리해주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers',\n",
       " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit',\n",
       " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history',\n",
       " 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan provides life cover age years also customers options insure critical illnesses disability accidental death benefit rider life cover age years',\n",
       " 'speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "for s in data['text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "    \n",
    "clean_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upgrad learner switches to career in ml al with salary hike',\n",
       " 'delhi techie wins free food from swiggy for one year on cred',\n",
       " 'new zealand end rohit sharma led india match winning streak',\n",
       " 'aegon life iterm insurance plan helps customers save tax',\n",
       " 'have known hirani for yrs what if metoo claims are not true sonam']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_headlines = []\n",
    "\n",
    "for s in data['headlines']:\n",
    "    clean_headlines.append(preprocess_sentence(s, False))\n",
    "    # 요약이므로 불용어를 제외할 경우 문장 자체가 이상해질 수 있음.\n",
    "\n",
    "clean_headlines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 칼럼에서 불용어를 모두 정리를 완료했으니 해당칼럼을 교체해주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_headlines\n",
    "\n",
    "# 불용어를 제거하면서 빈 값이 생겼을 수도 있으니\n",
    "# 빈 값을 Null 값으로 변환합니다.\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다행히 빈 샘플이 없습니다.  \n",
    "이제 샘플 문장의 최대길이를 설정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 60\n",
      "텍스트의 평균 길이 : 35.10029309397326\n",
      "텍스트의 표준 편차 : 3.7996134352887116\n",
      "헤드라인의 최소 길이 : 1\n",
      "헤드라인의 최대 길이 : 16\n",
      "헤드라인의 평균 길이 : 9.299444342675704\n",
      "헤드라인의 표준 편차 : 1.3899968497820723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "headlines_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('텍스트의 표준 편차 : {}'.format(np.std(text_len)))\n",
    "print('헤드라인의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
    "print('헤드라인의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
    "print('헤드라인의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
    "print('헤드라인의 표준 편차 : {}'.format(np.std(headlines_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(headlines_len)\n",
    "plt.title('headlines')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(headlines_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생각보다 데이터가 고르게 분포되어있습니다.  \n",
    "각 칼럼 샘플의 최대길이는 (평균+(2x표준편차로)) 결정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maxlen = int((np.mean(text_len)+(2*np.std(text_len))))\n",
    "headlines_maxlen = int((np.mean(headlines_len)+(2*np.std(headlines_len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 94658\n"
     ]
    }
   ],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_maxlen)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_maxlen)]\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>sostoken upgrad learner switches to career in ...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>india get all out for their lowest odi total i...</td>\n",
       "      <td>india recorded lowest odi total new zealand ge...</td>\n",
       "      <td>sostoken india get all out for their lowest od...</td>\n",
       "      <td>india get all out for their lowest odi total i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "5  rahat fateh ali khan denies getting notice for...   \n",
       "6  india get all out for their lowest odi total i...   \n",
       "\n",
       "                                                text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "2  new zealand defeated india wickets fourth odi ...   \n",
       "3  aegon life iterm insurance plan customers enjo...   \n",
       "5  pakistani singer rahat fateh ali khan denied r...   \n",
       "6  india recorded lowest odi total new zealand ge...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches to career in ...   \n",
       "2  sostoken new zealand end rohit sharma led indi...   \n",
       "3  sostoken aegon life iterm insurance plan helps...   \n",
       "5  sostoken rahat fateh ali khan denies getting n...   \n",
       "6  sostoken india get all out for their lowest od...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches to career in ml al wit...  \n",
       "2  new zealand end rohit sharma led india match w...  \n",
       "3  aegon life iterm insurance plan helps customer...  \n",
       "5  rahat fateh ali khan denies getting notice for...  \n",
       "6  india get all out for their lowest odi total i...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 18931\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 75727\n",
      "훈련 레이블의 개수 : 75727\n",
      "테스트 데이터의 개수 : 18931\n",
      "테스트 레이블의 개수 : 18931\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 67864\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 46163\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 21701\n",
      "단어 집합에서 희귀 단어의 비율: 68.02281032653542\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.5513729267157848\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 21000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10965, 7803, 354, 1124, 87, 22, 5945, 7353, 326, 8, 307, 2095, 1124, 10965, 1124, 982, 2234, 20204, 326, 8, 64, 285, 296, 10965, 3393, 296, 548, 1124, 1711, 1102], [73, 38, 13924, 2293, 2612, 9084, 8322, 395, 209, 1837, 7063, 5265, 826, 167, 387, 4767, 139, 488, 209, 68, 7063, 1441, 10966, 841, 62, 209, 68, 999, 594, 2, 10207, 12, 1564], [26, 78, 1185, 873, 63, 1, 413, 2265, 6503, 3377, 662, 140, 1042, 3602, 4, 253, 6577, 233, 25, 26, 1241, 729, 295, 2396, 8, 3377, 662, 55, 3394, 653]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "#잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 29597\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 19349\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 10248\n",
      "단어 집합에서 희귀 단어의 비율: 65.37486907456837\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.760733288424805\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 5740, 1643, 3, 18, 706, 981, 59, 168, 1594], [1, 4040, 160, 10, 1244, 451, 105, 3, 21, 7477, 34, 6030, 464], [1, 1540, 1166, 73, 10, 222, 26, 35, 456], [1, 148, 19, 4, 345, 9, 74, 815, 2812, 12, 1315], [1, 85, 970, 5741, 2326, 13, 397, 1421, 11, 59]]\n",
      "target\n",
      "decoder  [[5740, 1643, 3, 18, 706, 981, 59, 168, 1594, 2], [4040, 160, 10, 1244, 451, 105, 3, 21, 7477, 34, 6030, 464, 2], [1540, 1166, 73, 10, 222, 26, 35, 456, 2], [148, 19, 4, 345, 9, 74, 815, 2812, 12, 1315, 2], [85, 970, 5741, 2326, 13, 397, 1421, 11, 59, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 10000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "#잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 75727\n",
      "훈련 레이블의 개수 : 75727\n",
      "테스트 데이터의 개수 : 18931\n",
      "테스트 레이블의 개수 : 18931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_maxlen, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_maxlen, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = headlines_maxlen, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = headlines_maxlen, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = headlines_maxlen, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = headlines_maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 42, 128)      2688000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 42, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 42, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1280000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 42, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10000)  2570000     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,377,104\n",
      "Trainable params: 8,377,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_maxlen,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 42, 128)      2688000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 42, 256), (N 394240      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 42, 256), (N 525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 128)    1280000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 42, 256), (N 525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_7[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10000)  5130000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 11,068,432\n",
      "Trainable params: 11,068,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_maxlen,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model_attn = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model_attn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "296/296 [==============================] - 90s 304ms/step - loss: 5.7161 - val_loss: 5.3688\n",
      "Epoch 2/50\n",
      "296/296 [==============================] - 89s 302ms/step - loss: 5.3005 - val_loss: 5.1219\n",
      "Epoch 3/50\n",
      "296/296 [==============================] - 90s 303ms/step - loss: 5.0199 - val_loss: 4.8385\n",
      "Epoch 4/50\n",
      "296/296 [==============================] - 90s 304ms/step - loss: 4.7329 - val_loss: 4.6022\n",
      "Epoch 5/50\n",
      "296/296 [==============================] - 89s 302ms/step - loss: 4.5268 - val_loss: 4.4486\n",
      "Epoch 6/50\n",
      "296/296 [==============================] - 93s 314ms/step - loss: 4.3536 - val_loss: 4.3286\n",
      "Epoch 7/50\n",
      "296/296 [==============================] - 90s 303ms/step - loss: 4.2073 - val_loss: 4.2131\n",
      "Epoch 8/50\n",
      "296/296 [==============================] - 92s 311ms/step - loss: 4.0819 - val_loss: 4.1251\n",
      "Epoch 9/50\n",
      "296/296 [==============================] - 90s 304ms/step - loss: 3.9729 - val_loss: 4.0571\n",
      "Epoch 10/50\n",
      "296/296 [==============================] - 96s 325ms/step - loss: 3.8772 - val_loss: 3.9954\n",
      "Epoch 11/50\n",
      "296/296 [==============================] - 91s 309ms/step - loss: 3.7927 - val_loss: 3.9460\n",
      "Epoch 12/50\n",
      "296/296 [==============================] - 90s 302ms/step - loss: 3.7146 - val_loss: 3.9022\n",
      "Epoch 13/50\n",
      "296/296 [==============================] - 94s 318ms/step - loss: 3.6451 - val_loss: 3.8644\n",
      "Epoch 14/50\n",
      "296/296 [==============================] - 89s 300ms/step - loss: 3.5816 - val_loss: 3.8309\n",
      "Epoch 15/50\n",
      "296/296 [==============================] - 90s 305ms/step - loss: 3.5221 - val_loss: 3.8005\n",
      "Epoch 16/50\n",
      "296/296 [==============================] - 93s 315ms/step - loss: 3.4670 - val_loss: 3.7813\n",
      "Epoch 17/50\n",
      "296/296 [==============================] - 92s 311ms/step - loss: 3.4161 - val_loss: 3.7606\n",
      "Epoch 18/50\n",
      "296/296 [==============================] - 93s 316ms/step - loss: 3.3687 - val_loss: 3.7421\n",
      "Epoch 19/50\n",
      "296/296 [==============================] - 91s 308ms/step - loss: 3.3249 - val_loss: 3.7215\n",
      "Epoch 20/50\n",
      "296/296 [==============================] - 107s 361ms/step - loss: 3.2832 - val_loss: 3.7100\n",
      "Epoch 21/50\n",
      "296/296 [==============================] - 104s 353ms/step - loss: 3.2441 - val_loss: 3.6933\n",
      "Epoch 22/50\n",
      "296/296 [==============================] - 95s 322ms/step - loss: 3.2062 - val_loss: 3.6827\n",
      "Epoch 23/50\n",
      "296/296 [==============================] - 94s 319ms/step - loss: 3.1710 - val_loss: 3.6734\n",
      "Epoch 24/50\n",
      "296/296 [==============================] - 95s 321ms/step - loss: 3.1387 - val_loss: 3.6634\n",
      "Epoch 25/50\n",
      "296/296 [==============================] - 89s 301ms/step - loss: 3.1049 - val_loss: 3.6584\n",
      "Epoch 26/50\n",
      "296/296 [==============================] - 88s 296ms/step - loss: 3.0759 - val_loss: 3.6512\n",
      "Epoch 27/50\n",
      "296/296 [==============================] - 89s 301ms/step - loss: 3.0461 - val_loss: 3.6489\n",
      "Epoch 28/50\n",
      "296/296 [==============================] - 88s 297ms/step - loss: 3.0208 - val_loss: 3.6398\n",
      "Epoch 29/50\n",
      "296/296 [==============================] - 92s 309ms/step - loss: 2.9909 - val_loss: 3.6371\n",
      "Epoch 30/50\n",
      "296/296 [==============================] - 94s 318ms/step - loss: 2.9670 - val_loss: 3.6337\n",
      "Epoch 31/50\n",
      "296/296 [==============================] - 91s 309ms/step - loss: 2.9424 - val_loss: 3.6364\n",
      "Epoch 32/50\n",
      "296/296 [==============================] - 96s 323ms/step - loss: 2.9184 - val_loss: 3.6284\n",
      "Epoch 33/50\n",
      "296/296 [==============================] - 94s 317ms/step - loss: 2.8966 - val_loss: 3.6227\n",
      "Epoch 34/50\n",
      "296/296 [==============================] - 95s 321ms/step - loss: 2.8731 - val_loss: 3.6180\n",
      "Epoch 35/50\n",
      "296/296 [==============================] - 95s 321ms/step - loss: 2.8526 - val_loss: 3.6158\n",
      "Epoch 36/50\n",
      "296/296 [==============================] - 95s 321ms/step - loss: 2.8314 - val_loss: 3.6195\n",
      "Epoch 37/50\n",
      "296/296 [==============================] - 95s 319ms/step - loss: 2.8108 - val_loss: 3.6133\n",
      "Epoch 38/50\n",
      "296/296 [==============================] - 95s 320ms/step - loss: 2.7908 - val_loss: 3.6157\n",
      "Epoch 39/50\n",
      "296/296 [==============================] - 95s 320ms/step - loss: 2.7702 - val_loss: 3.6189\n",
      "Epoch 00039: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 256, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attn.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 256, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추상적 요약을 하는 경우에는 text를 본문, headlines를 이미 요약된 데이터로 삼아서 모델을 학습할 수 있어요. 추출적 요약을 하는 경우에는 오직 text열만을 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 2. 데이터 전처리하기 (추상적 요약)\n",
    "1. 실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 \n",
    "   추가 사용하여 텍스트를 정규화 또는 정제해 보세요.  \n",
    "2. 만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 \n",
    "   불용어를 제거하는 것이 좋을지 고민해보세요.  \n",
    "Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
    "일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 \n",
    "더 나은 성능을 얻을 수 있어요. \n",
    "실습 내용을 참고하여 어텐션 메커니즘을 사용한 seq2seq를 설계해 보세요.\n",
    "Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해보세요.  \n",
    "Step 5. Summa을 이용해서 추출적 요약해보기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
