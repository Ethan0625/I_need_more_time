{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Lyrics Generator\n",
    "### (작사가 인공지능 만들기)\n",
    "***\n",
    "**평가문항**  \n",
    ">***1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?  \n",
    "> -텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?***  \n",
    "> \n",
    "> ***2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?  \n",
    "> -특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?***\n",
    "> \n",
    "> ***3. 텍스트 생성모델이 안정적으로 학습되었는가?  \n",
    "> -텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?***\n",
    "\n",
    "***\n",
    "#### 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '', '[Spoken Intro:]', 'You ever want something ', \"that you know you shouldn't have \", \"The more you know you shouldn't have it, \", 'The more you want it ', 'And then one day you get it, ', \"It's so good too \", \"But it's just like my girl \", \"When she's around me \", 'I just feel so good, so good ', 'But right now I just feel cold, so cold ', 'Right down to my bones ', \"'Cause ooh... \", \"Ain't no sunshine when she's gone \", \"It's not warm when she's away \", \"Ain't no sunshine when she's gone \", \"And she's always gone too long \", 'Anytime she goes away ']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw) # 텍스트를 라인단위로 끊어서 리스트 형태로 읽어옴\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:20])\n",
    "# 특수문자들, 길이가 없는 문장 삭제 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2. 데이터 정제하기(데이터 전처리)\n",
    "- 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 텐서플로우\n",
    "import os\n",
    "\n",
    "# 문장 토큰화\n",
    "# 1. Hi,를 막기위해 문장부호 양쪽에 공백 추가\n",
    "# 2. 대소문자 중복을 막기위해 모든 문자를 소문자로 변환\n",
    "# 3. 특수문자로 인해 한단어로 인식되어 오류를 일으키는 것을 막기위해 특수문자들 제거\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 터링되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> step right up its circus time and get your moneys worth of love today <end>\n",
      "156013\n"
     ]
    }
   ],
   "source": [
    "# 1차 문장정리 데이터에 <start>와 <end>를 넣어 정리\n",
    "corpus = []\n",
    "new_corpus = []\n",
    "bad_corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "# 깔끔하게 정리된 문장에서 <start>와 <end>를 포함해서 문장길이가 15이하인 부분 삭제하기\n",
    "for i in range((len(corpus))):\n",
    "    if len(corpus[i].split(' ')) <=15 :\n",
    "        new_corpus.append(corpus[i])\n",
    "    else :\n",
    "        bad_corpus.append(corpus[i])\n",
    "\n",
    "print(bad_corpus[0])\n",
    "print(len(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156013, 15)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    # 단어장의 인덱스넘버가 12000개이고 띄어쓰기를 기준으로 토큰을 형성한다.\n",
    "    # 사전에 없는 단어는 <unk>으로 처리한다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  \n",
    "        # 전체 단어의 개수 \n",
    "        # 답으로 활용할 단어의 갯수라고 판단, 7000->12000개로 늘림\n",
    "        filters= ' ',    \n",
    "        # 별도로 전처리 로직을 추가할 수 있다.\n",
    "        # ex ) !\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\n",
    "        oov_token=\"<unk>\"  \n",
    "        # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지 결정\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    # 문장의 최대길이를 조절하여 토큰의 갯수를 조절한다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor.shape)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 셋 분리\n",
    "# 소스문장(<start> blah blah <pad><pad><pad>)\n",
    "# 타겟문장(blah blah <end> <pad><pad><pad>)\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. \n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 3. 평가 데이터셋 분리\n",
    "- 단어장의 크기는 12000 이상으로 설정.\n",
    "- 총 데이터의 20%를 평가 데이터셋으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 총데이터를 학습데이터와 평가용 데이터셋으로 나눔(train_test_split())활용.\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (124810, 14)\n",
      "Target Train:  (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 4. 인공지능 만들기\n",
    "- 10Epoch안에 val_loss를 2.2수준으로 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 8.00087801e-05,  2.65578565e-04, -2.63377180e-04, ...,\n",
       "         -2.40562149e-04, -1.70083600e-04, -4.41776174e-05],\n",
       "        [ 3.03188950e-04,  1.93652100e-04, -3.17938364e-04, ...,\n",
       "         -4.62011260e-04, -3.63254017e-04, -1.40514414e-04],\n",
       "        ...,\n",
       "        [ 1.14587579e-04,  1.41202484e-03, -2.62370359e-05, ...,\n",
       "         -7.27077771e-04, -6.42314204e-04, -1.17238975e-04],\n",
       "        [ 7.24197307e-05,  1.17003743e-03,  1.74008237e-04, ...,\n",
       "         -9.42305080e-04, -6.87676584e-05, -2.44093593e-04],\n",
       "        [-3.50925002e-05,  8.35430052e-04,  2.73941754e-04, ...,\n",
       "         -1.14611140e-03,  5.88991388e-04, -3.80493264e-04]],\n",
       "\n",
       "       [[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 7.45294471e-07,  4.49933112e-04, -4.10763751e-04, ...,\n",
       "          3.87860528e-05, -9.82756974e-05, -1.11153880e-04],\n",
       "        [ 1.28359578e-04,  3.93227907e-04, -5.72735793e-04, ...,\n",
       "         -3.89049819e-05, -1.74948727e-04,  1.79959767e-04],\n",
       "        ...,\n",
       "        [-6.45365682e-04,  1.26498009e-04,  5.75608574e-04, ...,\n",
       "         -2.01929972e-04,  5.82821376e-05, -3.61902930e-04],\n",
       "        [-4.37965966e-04,  2.79265980e-04,  7.78832298e-04, ...,\n",
       "         -1.50273438e-04,  2.36669639e-05, -3.78576835e-04],\n",
       "        [-3.64869309e-04,  1.52643712e-04,  9.58752295e-04, ...,\n",
       "         -2.57417734e-04,  2.52605096e-04, -4.78300150e-04]],\n",
       "\n",
       "       [[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 4.45538972e-05,  1.66884536e-04,  2.56968506e-05, ...,\n",
       "         -1.27235893e-04, -2.56020558e-05, -3.60901584e-04],\n",
       "        [-1.37727448e-05,  2.02558382e-04, -2.64899263e-05, ...,\n",
       "         -1.70598738e-04, -5.92460165e-05, -4.14440001e-04],\n",
       "        ...,\n",
       "        [ 8.54554877e-04, -7.53259985e-04,  1.10736422e-04, ...,\n",
       "         -1.18787005e-03,  1.30504835e-03, -5.45074465e-04],\n",
       "        [ 6.08372444e-04, -1.00462604e-03,  1.17370466e-04, ...,\n",
       "         -1.35183753e-03,  1.85676140e-03, -6.82426151e-04],\n",
       "        [ 3.47174471e-04, -1.23542326e-03,  6.11257638e-05, ...,\n",
       "         -1.47136836e-03,  2.40802416e-03, -7.93989748e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 1.14597729e-04,  2.27705386e-05, -1.76346206e-04, ...,\n",
       "         -2.60165107e-04, -1.10161352e-04,  2.81976740e-04],\n",
       "        [ 2.02527066e-04, -2.30474529e-04, -2.16012966e-04, ...,\n",
       "         -3.16122198e-04,  1.73119901e-04,  5.03059186e-04],\n",
       "        ...,\n",
       "        [-4.33082372e-04, -1.99643476e-03, -1.69801642e-04, ...,\n",
       "         -1.23176107e-03,  3.96251585e-03, -5.22750081e-04],\n",
       "        [-6.04336674e-04, -2.14195042e-03, -3.66435212e-04, ...,\n",
       "         -1.28763635e-03,  4.28943662e-03, -5.72654884e-04],\n",
       "        [-7.49255647e-04, -2.26383260e-03, -5.56099054e-04, ...,\n",
       "         -1.32998941e-03,  4.58195154e-03, -6.03896100e-04]],\n",
       "\n",
       "       [[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 1.32445741e-04,  2.00883616e-04,  4.21174300e-05, ...,\n",
       "          2.32849907e-05, -2.19768954e-05, -1.64865865e-04],\n",
       "        [ 1.68605795e-04,  2.16496250e-04,  2.70985533e-04, ...,\n",
       "          2.88371870e-04, -1.01880003e-04,  3.57015524e-05],\n",
       "        ...,\n",
       "        [-3.72240640e-04,  6.79953300e-05, -4.41729971e-05, ...,\n",
       "          2.30250167e-04,  3.94853560e-04, -6.46452827e-04],\n",
       "        [-4.53359447e-04, -7.43066630e-05,  1.62603625e-04, ...,\n",
       "          2.50017511e-05,  7.16411101e-04, -7.31615874e-04],\n",
       "        [-5.85588103e-04, -3.27064190e-04,  3.33578908e-04, ...,\n",
       "         -2.56185973e-04,  1.15320331e-03, -8.38766515e-04]],\n",
       "\n",
       "       [[-1.30229646e-05,  1.65365869e-04, -1.22482495e-04, ...,\n",
       "         -8.87530623e-05, -6.59474463e-05, -2.39839756e-05],\n",
       "        [ 2.73953592e-05,  8.75899495e-05, -4.70092418e-05, ...,\n",
       "         -5.35552063e-05, -3.45206063e-05, -1.67410239e-04],\n",
       "        [ 9.53207200e-05,  1.62127708e-05, -1.74138666e-04, ...,\n",
       "         -9.20349412e-05,  1.31175941e-04, -1.52335297e-05],\n",
       "        ...,\n",
       "        [ 9.59047349e-04,  1.84814341e-03, -1.25287392e-03, ...,\n",
       "         -9.53182403e-04, -5.71393466e-04,  3.05929017e-04],\n",
       "        [ 9.38262034e-04,  1.74549036e-03, -1.00995251e-03, ...,\n",
       "         -1.04222877e-03, -3.50584683e-04,  1.91244661e-04],\n",
       "        [ 7.99334433e-04,  1.45031954e-03, -7.83238793e-04, ...,\n",
       "         -1.20144349e-03,  6.97164141e-05,  1.13390070e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 3.4966\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 102s 167ms/step - loss: 2.9979\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.8197\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 102s 167ms/step - loss: 2.6868\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 103s 169ms/step - loss: 2.5729\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 103s 169ms/step - loss: 2.4680\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 107s 176ms/step - loss: 2.3705\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 108s 178ms/step - loss: 2.2791\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 104s 170ms/step - loss: 2.1935\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 104s 170ms/step - loss: 2.1128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f712ffe5c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976/976 - 10s - loss: 2.0186\n",
      "2.0185556411743164\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val, dec_val, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **결과**\n",
    "> **train losdd : 2.1**  \n",
    "> **validation loss : 2.0186**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate to see your face <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **결론**\n",
    "> **다운받은 Song Lyrics 데이터의 크기 : 187088**  \n",
    "> **문장 토큰화 결과 (문장부호 공백추가, 모든문자 소문자로 변환, 특수문자 제거 (156013, 15)로 변환하였음.  \n",
    "> 이를 학습데이터와 검증 데이터로 나누어 학습 후 검증하였음.  \n",
    "> 학습 시 => train losdd : 2.1  \n",
    "> 검증결과 => validation loss : 2.0186 로 잘 학습 되었음.  \n",
    "> 해당 모델로 문장 생성 실시.**  \n",
    "> **'i hate' 이라는 문장을 시작문자로 줬을 때 'i hate to see your face' 문장 생성.**\n",
    "***\n",
    "#### **회고록**\n",
    "> **작사를 하는 인공지능을 만드는 건 생각보다 재밌었다.  \n",
    "> 단어뭉치들을 학습해서 문장을 만들다니!  \n",
    "> 이런 기술들이 발전하고 발전하면 정말 음성으로 대화가 가능한 인공지능이 가능할 수도 있을 것 같다.  \n",
    "> 그런 기술이 상용화되었을 때 그 기술의 한복판에 내가 있었으면 좋겠다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
