{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 - 멋진 챗봇 만들기\n",
    "### Transformer Chatbot\n",
    "***\n",
    "#### Step 1. 데이터 다운로드\n",
    "\n",
    "#### Step 2. 데이터 정제\n",
    "\n",
    "#### Step 3. 데이터 토큰화\n",
    "\n",
    "#### Step 4. Augmentation\n",
    "\n",
    "#### Step 5. 데이터 벡터화\n",
    "\n",
    "#### Step 6. 훈련하기\n",
    "\n",
    "#### Step 7. 성능 측정하기\n",
    "***\n",
    "### 평가문항\n",
    "**1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?**  \n",
    "-챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.  \n",
    "\n",
    "**2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?**  \n",
    "-과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.  \n",
    "\n",
    "**3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?**  \n",
    "-주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.  \n",
    "***\n",
    "#### Step 1. 데이터 다운로드\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "import gensim\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/aiffel/aiffel/transformer_chatbot/ChatbotData .csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['Q']\n",
    "answers = data['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             12시 땡!\n",
      "1        1지망 학교 떨어졌어\n",
      "2       3박4일 놀러가고 싶다\n",
      "3    3박4일 정도 놀러가고 싶다\n",
      "4            PPL 심하네\n",
      "Name: Q, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(questions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     하루가 또 가네요.\n",
      "1      위로해 드립니다.\n",
      "2    여행은 언제나 좋죠.\n",
      "3    여행은 언제나 좋죠.\n",
      "4     눈살이 찌푸려지죠.\n",
      "Name: A, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(answers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    if sentence.isalpha():\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎ|가-힣|a-zA-Z|0-9|?!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src, tgt):\n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "    \n",
    "    for idx, sentence in enumerate(src):\n",
    "        src_preprocessed = preprocess_sentence(sentence)\n",
    "        tgt_preprocessed = preprocess_sentence(tgt[idx])\n",
    "        \n",
    "        if src_preprocessed in que_corpus or tgt_preprocessed in ans_corpus:continue\n",
    "        else:\n",
    "            src_tokenized = mecab.morphs(src_preprocessed)\n",
    "            tgt_tokenized = mecab.morphs(tgt_preprocessed)\n",
    "            \n",
    "            if len(src_tokenized) <= 20 and len(tgt_tokenized) <= 20:\n",
    "                que_corpus.append(src_tokenized)\n",
    "                ans_corpus.append(tgt_tokenized)\n",
    "            else:continue\n",
    "    \n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시', '땡', '!'],\n",
       " ['1', '지망', '학교', '떨어졌', '어'],\n",
       " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['PPL', '심하', '네']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요'],\n",
       " ['위로', '해', '드립니다'],\n",
       " ['여행', '은', '언제나', '좋', '죠'],\n",
       " ['여행', '은', '언제나', '좋', '죠'],\n",
       " ['눈살', '이', '찌푸려', '지', '죠']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11713\n",
      "11713\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec.load('/home/aiffel/aiffel/transformer_chatbot/ko.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12 시가 땡 ! '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_sub(que_corpus[0], wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6e829817da412e91c5ea742004733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e754c7cf11147e78d2bf0abaf3d0251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], wv)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "    else:continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], wv)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "    else:continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948\n",
      "19948\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시가', '땡', '!'],\n",
       " ['3', '김', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['3', '김', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['PPL', '강하', '네'],\n",
       " ['SD', '카드', '망가졌', '어서']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_que_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요'],\n",
       " ['여행', '은', '언제나', '좋', '죠'],\n",
       " ['여행', '은', '언제나', '좋', '죠'],\n",
       " ['눈살', '이', '찌푸려', '지', '죠'],\n",
       " ['다시', '새로', '사', '는', '게', '마음', '편', '해요']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    temp.append([\"<start>\"] + corpus + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_corpus = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start>', '하루', '가', '또', '가', '네요', '<end>'],\n",
       " ['<start>', '위로', '해', '드립니다', '<end>'],\n",
       " ['<start>', '여행', '은', '언제나', '좋', '죠', '<end>'],\n",
       " ['<start>', '여행', '은', '언제나', '좋', '죠', '<end>'],\n",
       " ['<start>', '눈살', '이', '찌푸려', '지', '죠', '<end>']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23426"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = que_corpus + ans_corpus\n",
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.concatenate(total_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '<start>': 2,\n",
       " '<end>': 3,\n",
       " '이': 4,\n",
       " '하': 5,\n",
       " '는': 6,\n",
       " '을': 7,\n",
       " '세요': 8,\n",
       " '가': 9,\n",
       " '어': 10,\n",
       " '고': 11,\n",
       " '좋': 12,\n",
       " '거': 13,\n",
       " '해': 14,\n",
       " '있': 15,\n",
       " '보': 16,\n",
       " '은': 17,\n",
       " '지': 18,\n",
       " '?': 19,\n",
       " '나': 20,\n",
       " '아': 21,\n",
       " '도': 22,\n",
       " '게': 23,\n",
       " '에': 24,\n",
       " '겠': 25,\n",
       " '예요': 26,\n",
       " '사람': 27,\n",
       " '어요': 28,\n",
       " '다': 29,\n",
       " '를': 30,\n",
       " '한': 31,\n",
       " '같': 32,\n",
       " '죠': 33,\n",
       " '사랑': 34,\n",
       " '네요': 35,\n",
       " '싶': 36,\n",
       " '면': 37,\n",
       " '수': 38,\n",
       " '안': 39,\n",
       " '네': 40,\n",
       " '없': 41,\n",
       " '생각': 42,\n",
       " '친구': 43,\n",
       " '것': 44,\n",
       " '의': 45,\n",
       " '잘': 46,\n",
       " '아요': 47,\n",
       " '봐요': 48,\n",
       " '말': 49,\n",
       " '할': 50,\n",
       " '는데': 51,\n",
       " '않': 52,\n",
       " '마음': 53,\n",
       " '너무': 54,\n",
       " '되': 55,\n",
       " '주': 56,\n",
       " '했': 57,\n",
       " '만': 58,\n",
       " '일': 59,\n",
       " '기': 60,\n",
       " '더': 61,\n",
       " '이별': 62,\n",
       " '었': 63,\n",
       " '내': 64,\n",
       " '들': 65,\n",
       " '연락': 66,\n",
       " '여자': 67,\n",
       " '남자': 68,\n",
       " '힘들': 69,\n",
       " '해요': 70,\n",
       " '시간': 71,\n",
       " '많이': 72,\n",
       " '길': 73,\n",
       " '으면': 74,\n",
       " '요': 75,\n",
       " '먹': 76,\n",
       " '좀': 77,\n",
       " '남': 78,\n",
       " '에요': 79,\n",
       " '에서': 80,\n",
       " '으로': 81,\n",
       " '한테': 82,\n",
       " '썸': 83,\n",
       " '때': 84,\n",
       " '!': 85,\n",
       " '았': 86,\n",
       " '많': 87,\n",
       " '야': 88,\n",
       " '짝': 89,\n",
       " '저': 90,\n",
       " '받': 91,\n",
       " '건': 92,\n",
       " '뭐': 93,\n",
       " '오늘': 94,\n",
       " '만나': 95,\n",
       " '로': 96,\n",
       " '마세요': 97,\n",
       " '을까': 98,\n",
       " '알': 99,\n",
       " '적': 100,\n",
       " '에게': 101,\n",
       " '해도': 102,\n",
       " '괜찮': 103,\n",
       " '습니다': 104,\n",
       " '애': 105,\n",
       " '못': 106,\n",
       " '그': 107,\n",
       " '이제': 108,\n",
       " '인': 109,\n",
       " '아니': 110,\n",
       " '자신': 111,\n",
       " '연애': 112,\n",
       " '할까': 113,\n",
       " '필요': 114,\n",
       " '끝': 115,\n",
       " '랑': 116,\n",
       " '왜': 117,\n",
       " '던': 118,\n",
       " '해야': 119,\n",
       " '모르': 120,\n",
       " '살': 121,\n",
       " '라고': 122,\n",
       " '걸': 123,\n",
       " '다른': 124,\n",
       " '년': 125,\n",
       " '지만': 126,\n",
       " '달': 127,\n",
       " '타': 128,\n",
       " '혼자': 129,\n",
       " '잊': 130,\n",
       " '당신': 131,\n",
       " '다시': 132,\n",
       " '하나': 133,\n",
       " '어떻게': 134,\n",
       " '데': 135,\n",
       " '오': 136,\n",
       " '돼': 137,\n",
       " '정리': 138,\n",
       " '전': 139,\n",
       " '될': 140,\n",
       " '지금': 141,\n",
       " '날': 142,\n",
       " '또': 143,\n",
       " '정말': 144,\n",
       " '제': 145,\n",
       " '중': 146,\n",
       " '서': 147,\n",
       " '봐': 148,\n",
       " '과': 149,\n",
       " '라': 150,\n",
       " '싫': 151,\n",
       " '같이': 152,\n",
       " '인데': 153,\n",
       " '중요': 154,\n",
       " '결혼': 155,\n",
       " '인가': 156,\n",
       " '사': 157,\n",
       " '행복': 158,\n",
       " '니': 159,\n",
       " '조금': 160,\n",
       " '준비': 161,\n",
       " '은데': 162,\n",
       " '줄': 163,\n",
       " '와': 164,\n",
       " '아직': 165,\n",
       " '이랑': 166,\n",
       " '참': 167,\n",
       " '고민': 168,\n",
       " '면서': 169,\n",
       " '짝사랑': 170,\n",
       " '게요': 171,\n",
       " '자꾸': 172,\n",
       " '집': 173,\n",
       " '물': 174,\n",
       " '시': 175,\n",
       " '보다': 176,\n",
       " '선물': 177,\n",
       " '방법': 178,\n",
       " '돼요': 179,\n",
       " '왔': 180,\n",
       " '합니다': 181,\n",
       " '맞': 182,\n",
       " '그런': 183,\n",
       " '까지': 184,\n",
       " '바랄': 185,\n",
       " '진짜': 186,\n",
       " '후회': 187,\n",
       " '이야기': 188,\n",
       " '는지': 189,\n",
       " '고백': 190,\n",
       " '쓰': 191,\n",
       " '술': 192,\n",
       " '다고': 193,\n",
       " '꿈': 194,\n",
       " '먼저': 195,\n",
       " '헤어진지': 196,\n",
       " '봅니다': 197,\n",
       " '될까': 198,\n",
       " '시작': 199,\n",
       " '녀': 200,\n",
       " '헤어지': 201,\n",
       " '자': 202,\n",
       " '사귀': 203,\n",
       " '다면': 204,\n",
       " '드세요': 205,\n",
       " '힘든': 206,\n",
       " '됐': 207,\n",
       " '서로': 208,\n",
       " '놀': 209,\n",
       " '해서': 210,\n",
       " '그녀': 211,\n",
       " '번': 212,\n",
       " '공부': 213,\n",
       " '감정': 214,\n",
       " '계속': 215,\n",
       " '그럴': 216,\n",
       " '듯': 217,\n",
       " '가능': 218,\n",
       " '쉬': 219,\n",
       " '든': 220,\n",
       " '라도': 221,\n",
       " '돈': 222,\n",
       " '기분': 223,\n",
       " '힘드': 224,\n",
       " '눈': 225,\n",
       " '볼까': 226,\n",
       " '곳': 227,\n",
       " '여친': 228,\n",
       " '신경': 229,\n",
       " '만큼': 230,\n",
       " '만들': 231,\n",
       " '두': 232,\n",
       " '걱정': 233,\n",
       " '바랍니다': 234,\n",
       " '러': 235,\n",
       " '여행': 236,\n",
       " '좋아하': 237,\n",
       " '이상': 238,\n",
       " '씩': 239,\n",
       " '데이트': 240,\n",
       " '후': 241,\n",
       " '자고': 242,\n",
       " '쉽': 243,\n",
       " '입니다': 244,\n",
       " '째': 245,\n",
       " '지요': 246,\n",
       " '어도': 247,\n",
       " '너': 248,\n",
       " '려고': 249,\n",
       " '하루': 250,\n",
       " '남친': 251,\n",
       " '기억': 252,\n",
       " '인지': 253,\n",
       " '상처': 254,\n",
       " '표현': 255,\n",
       " '이렇게': 256,\n",
       " '맛있': 257,\n",
       " '무슨': 258,\n",
       " '카톡': 259,\n",
       " '걸까': 260,\n",
       " '나요': 261,\n",
       " '바라': 262,\n",
       " '니까': 263,\n",
       " '2': 264,\n",
       " '헤어졌': 265,\n",
       " '니까요': 266,\n",
       " '줘': 267,\n",
       " '기다리': 268,\n",
       " '그냥': 269,\n",
       " '대화': 270,\n",
       " '머리': 271,\n",
       " '다가': 272,\n",
       " '믿': 273,\n",
       " '함께': 274,\n",
       " '1': 275,\n",
       " '긴': 276,\n",
       " '이해': 277,\n",
       " '어떨까': 278,\n",
       " '어서': 279,\n",
       " '전화': 280,\n",
       " '몰라요': 281,\n",
       " '결정': 282,\n",
       " '텐데': 283,\n",
       " '내일': 284,\n",
       " '누구': 285,\n",
       " '어떤': 286,\n",
       " ',': 287,\n",
       " '새로운': 288,\n",
       " '셨': 289,\n",
       " '어디': 290,\n",
       " '관심': 291,\n",
       " '님': 292,\n",
       " '선택': 293,\n",
       " '아프': 294,\n",
       " '썸남': 295,\n",
       " '3': 296,\n",
       " '궁금': 297,\n",
       " '된': 298,\n",
       " '함': 299,\n",
       " '운동': 300,\n",
       " '헤어진': 301,\n",
       " '나가': 302,\n",
       " '부담': 303,\n",
       " '회사': 304,\n",
       " '맘': 305,\n",
       " '입': 306,\n",
       " '라면': 307,\n",
       " '보내': 308,\n",
       " '잠': 309,\n",
       " '비': 310,\n",
       " '속': 311,\n",
       " '죽': 312,\n",
       " '미련': 313,\n",
       " '때문': 314,\n",
       " '이유': 315,\n",
       " '오래': 316,\n",
       " '노력': 317,\n",
       " '부터': 318,\n",
       " '개월': 319,\n",
       " '처럼': 320,\n",
       " '우리': 321,\n",
       " '생겼': 322,\n",
       " '인연': 323,\n",
       " '직접': 324,\n",
       " '드': 325,\n",
       " '앞': 326,\n",
       " '언제': 327,\n",
       " '한가': 328,\n",
       " '차': 329,\n",
       " '어떡': 330,\n",
       " '늦': 331,\n",
       " '대로': 332,\n",
       " '만날': 333,\n",
       " '이젠': 334,\n",
       " '스트레스': 335,\n",
       " '잘못': 336,\n",
       " '였': 337,\n",
       " '따라': 338,\n",
       " '인생': 339,\n",
       " '항상': 340,\n",
       " '그게': 341,\n",
       " '도움': 342,\n",
       " '으세요': 343,\n",
       " '음': 344,\n",
       " '군요': 345,\n",
       " '반': 346,\n",
       " '잡': 347,\n",
       " '용기': 348,\n",
       " '올': 349,\n",
       " '뿐': 350,\n",
       " '거나': 351,\n",
       " '찾아보': 352,\n",
       " '상황': 353,\n",
       " '느낌': 354,\n",
       " '라는': 355,\n",
       " '축하': 356,\n",
       " '이나': 357,\n",
       " '그렇게': 358,\n",
       " '문제': 359,\n",
       " '가지': 360,\n",
       " '변화': 361,\n",
       " '첫': 362,\n",
       " '만났': 363,\n",
       " '기대': 364,\n",
       " '조심': 365,\n",
       " '답답': 366,\n",
       " '건가': 367,\n",
       " '대': 368,\n",
       " '처음': 369,\n",
       " '추억': 370,\n",
       " '가보세요': 371,\n",
       " '감': 372,\n",
       " '사이': 373,\n",
       " '결국': 374,\n",
       " '다음': 375,\n",
       " '힘': 376,\n",
       " '해봐요': 377,\n",
       " '매일': 378,\n",
       " '현실': 379,\n",
       " '요즘': 380,\n",
       " '나이': 381,\n",
       " '노래': 382,\n",
       " '스럽': 383,\n",
       " '충분히': 384,\n",
       " '아서': 385,\n",
       " '건강': 386,\n",
       " '습관': 387,\n",
       " '다는': 388,\n",
       " '볼': 389,\n",
       " '분': 390,\n",
       " '세상': 391,\n",
       " '꼭': 392,\n",
       " '어야': 393,\n",
       " '못하': 394,\n",
       " '질': 395,\n",
       " '열심히': 396,\n",
       " '만남': 397,\n",
       " '순간': 398,\n",
       " '찾': 399,\n",
       " '마지막': 400,\n",
       " '추천': 401,\n",
       " '챙겨': 402,\n",
       " '보이': 403,\n",
       " '봤': 404,\n",
       " '상대': 405,\n",
       " '놓': 406,\n",
       " '건지': 407,\n",
       " '났': 408,\n",
       " '의미': 409,\n",
       " '얼른': 410,\n",
       " '말씀': 411,\n",
       " '힘내': 412,\n",
       " '가슴': 413,\n",
       " '관계': 414,\n",
       " '그만': 415,\n",
       " '몸': 416,\n",
       " '아무': 417,\n",
       " '넘': 418,\n",
       " '지내': 419,\n",
       " '확인': 420,\n",
       " '나쁜': 421,\n",
       " '일까': 422,\n",
       " '난': 423,\n",
       " '큰': 424,\n",
       " '덜': 425,\n",
       " '아침': 426,\n",
       " '상대방': 427,\n",
       " '으니까요': 428,\n",
       " '갈': 429,\n",
       " '까': 430,\n",
       " '귀찮': 431,\n",
       " '화': 432,\n",
       " '밥': 433,\n",
       " '제일': 434,\n",
       " '한지': 435,\n",
       " '재회': 436,\n",
       " '헤어짐': 437,\n",
       " '정도': 438,\n",
       " '이런': 439,\n",
       " '몇': 440,\n",
       " '다니': 441,\n",
       " '마시': 442,\n",
       " '가요': 443,\n",
       " '어때': 444,\n",
       " '줬': 445,\n",
       " '부모': 446,\n",
       " '호감': 447,\n",
       " '그렇': 448,\n",
       " '진심': 449,\n",
       " '아닌': 450,\n",
       " '그러': 451,\n",
       " '새': 452,\n",
       " '성공': 453,\n",
       " '핸드폰': 454,\n",
       " '웃': 455,\n",
       " '영화': 456,\n",
       " '어제': 457,\n",
       " '커피': 458,\n",
       " '없이': 459,\n",
       " '선': 460,\n",
       " '졌': 461,\n",
       " '사진': 462,\n",
       " '주말': 463,\n",
       " '둘': 464,\n",
       " '모든': 465,\n",
       " '엄마': 466,\n",
       " '연습': 467,\n",
       " '마다': 468,\n",
       " '자연': 469,\n",
       " '을까요': 470,\n",
       " '아파': 471,\n",
       " '다르': 472,\n",
       " '듣': 473,\n",
       " '드릴게요': 474,\n",
       " '가끔': 475,\n",
       " '해야지': 476,\n",
       " '재밌': 477,\n",
       " '그래도': 478,\n",
       " '딱': 479,\n",
       " '뭘': 480,\n",
       " '날씨': 481,\n",
       " '아도': 482,\n",
       " '복잡': 483,\n",
       " '스스로': 484,\n",
       " '모두': 485,\n",
       " '다가가': 486,\n",
       " '갖': 487,\n",
       " '마': 488,\n",
       " '동안': 489,\n",
       " '얼굴': 490,\n",
       " '답': 491,\n",
       " '글': 492,\n",
       " '짧': 493,\n",
       " '자기': 494,\n",
       " '확신': 495,\n",
       " '가장': 496,\n",
       " '후폭풍': 497,\n",
       " '갑자기': 498,\n",
       " '눈물': 499,\n",
       " '실수': 500,\n",
       " '짜증': 501,\n",
       " '소개팅': 502,\n",
       " '란': 503,\n",
       " '예의': 504,\n",
       " '버리': 505,\n",
       " '가져': 506,\n",
       " '약': 507,\n",
       " '대한': 508,\n",
       " '관리': 509,\n",
       " '법': 510,\n",
       " '엄청': 511,\n",
       " '기다려': 512,\n",
       " '남편': 513,\n",
       " '피곤': 514,\n",
       " '위로': 515,\n",
       " '톡': 516,\n",
       " '아픔': 517,\n",
       " '갔': 518,\n",
       " '편하': 519,\n",
       " '더니': 520,\n",
       " '정신': 521,\n",
       " '뭘까': 522,\n",
       " '아야': 523,\n",
       " '바람': 524,\n",
       " '5': 525,\n",
       " '차단': 526,\n",
       " '받아들이': 527,\n",
       " '학교': 528,\n",
       " '게임': 529,\n",
       " '붙잡': 530,\n",
       " '예쁘': 531,\n",
       " '봄': 532,\n",
       " '스러운': 533,\n",
       " '냐': 534,\n",
       " '편': 535,\n",
       " '쉬운': 536,\n",
       " '아픈': 537,\n",
       " '자주': 538,\n",
       " '확실': 539,\n",
       " '천천히': 540,\n",
       " '4': 541,\n",
       " '생활': 542,\n",
       " '옷': 543,\n",
       " '빨리': 544,\n",
       " '점점': 545,\n",
       " '나와': 546,\n",
       " '밤': 547,\n",
       " '벌써': 548,\n",
       " '따뜻': 549,\n",
       " '극복': 550,\n",
       " '티': 551,\n",
       " '인정': 552,\n",
       " '환승': 553,\n",
       " '모습': 554,\n",
       " '보여': 555,\n",
       " '감기': 556,\n",
       " '무시': 557,\n",
       " '시켜': 558,\n",
       " '운명': 559,\n",
       " '별로': 560,\n",
       " '을지': 561,\n",
       " '6': 562,\n",
       " '충분': 563,\n",
       " '어야지': 564,\n",
       " '나오': 565,\n",
       " '솔직': 566,\n",
       " '바쁘': 567,\n",
       " '잠시': 568,\n",
       " '주변': 569,\n",
       " '재미': 570,\n",
       " '찍': 571,\n",
       " '성격': 572,\n",
       " '일찍': 573,\n",
       " '울': 574,\n",
       " '집착': 575,\n",
       " '지났': 576,\n",
       " '직장': 577,\n",
       " '만난': 578,\n",
       " '진': 579,\n",
       " '언젠간': 580,\n",
       " '행동': 581,\n",
       " '사세요': 582,\n",
       " '가족': 583,\n",
       " '폰': 584,\n",
       " '도전': 585,\n",
       " '아닌데': 586,\n",
       " '원': 587,\n",
       " '화장': 588,\n",
       " '설레': 589,\n",
       " '일어나': 590,\n",
       " '잔': 591,\n",
       " '마련': 592,\n",
       " '얼마': 593,\n",
       " '형': 594,\n",
       " '익숙': 595,\n",
       " '슬픈': 596,\n",
       " '그분': 597,\n",
       " '풀': 598,\n",
       " '소리': 599,\n",
       " '도와': 600,\n",
       " '낫': 601,\n",
       " '떠나': 602,\n",
       " '여유': 603,\n",
       " '후련': 604,\n",
       " '척': 605,\n",
       " '포기': 606,\n",
       " '크': 607,\n",
       " '무엇': 608,\n",
       " '최고': 609,\n",
       " '생각나': 610,\n",
       " '여기': 611,\n",
       " '아무것': 612,\n",
       " '별': 613,\n",
       " '할지': 614,\n",
       " '점': 615,\n",
       " '아무래도': 616,\n",
       " '자체': 617,\n",
       " '곧': 618,\n",
       " '잊혀': 619,\n",
       " '다행': 620,\n",
       " '거짓말': 621,\n",
       " '지나': 622,\n",
       " '야지': 623,\n",
       " '비싸': 624,\n",
       " '바': 625,\n",
       " '나중': 626,\n",
       " '생일': 627,\n",
       " '능력': 628,\n",
       " '손': 629,\n",
       " '삶': 630,\n",
       " '한다고': 631,\n",
       " '카페': 632,\n",
       " '차이': 633,\n",
       " '안녕': 634,\n",
       " '편지': 635,\n",
       " '위해': 636,\n",
       " '연인': 637,\n",
       " '알아보': 638,\n",
       " '주무세요': 639,\n",
       " '방학': 640,\n",
       " '스러워': 641,\n",
       " '귀': 642,\n",
       " '맨날': 643,\n",
       " '보통': 644,\n",
       " '싸우': 645,\n",
       " '준': 646,\n",
       " '시원': 647,\n",
       " '성': 648,\n",
       " '즐기': 649,\n",
       " '별후': 650,\n",
       " '소식': 651,\n",
       " '겠죠': 652,\n",
       " '재미있': 653,\n",
       " '시험': 654,\n",
       " '어렵': 655,\n",
       " '한데': 656,\n",
       " '임': 657,\n",
       " '상관': 658,\n",
       " '스타일': 659,\n",
       " '부족': 660,\n",
       " '하늘': 661,\n",
       " '상담': 662,\n",
       " '간': 663,\n",
       " '작': 664,\n",
       " '우산': 665,\n",
       " '드리': 666,\n",
       " '소중': 667,\n",
       " '잠깐': 668,\n",
       " '읽': 669,\n",
       " '흘렀': 670,\n",
       " '변하': 671,\n",
       " '응원': 672,\n",
       " '살펴보': 673,\n",
       " '다를': 674,\n",
       " '금방': 675,\n",
       " '고생': 676,\n",
       " '기회': 677,\n",
       " '배우': 678,\n",
       " '완전': 679,\n",
       " '얘기': 680,\n",
       " '실': 681,\n",
       " '쯤': 682,\n",
       " '미안': 683,\n",
       " '며': 684,\n",
       " '진정': 685,\n",
       " '세': 686,\n",
       " '우울': 687,\n",
       " '동거': 688,\n",
       " '깊': 689,\n",
       " '한다는': 690,\n",
       " '해질': 691,\n",
       " '언제나': 692,\n",
       " '자책': 693,\n",
       " '드려요': 694,\n",
       " '겠지': 695,\n",
       " '욕': 696,\n",
       " '줄까': 697,\n",
       " '프': 698,\n",
       " '구': 699,\n",
       " '약속': 700,\n",
       " '미치': 701,\n",
       " '깨': 702,\n",
       " '바로': 703,\n",
       " '접': 704,\n",
       " '여': 705,\n",
       " '문자': 706,\n",
       " '즐거운': 707,\n",
       " '허전': 708,\n",
       " '과정': 709,\n",
       " '는다면': 710,\n",
       " '아닌지': 711,\n",
       " '그래요': 712,\n",
       " '생길': 713,\n",
       " '달라지': 714,\n",
       " '나왔': 715,\n",
       " '간다': 716,\n",
       " '갈까': 717,\n",
       " '취미': 718,\n",
       " '번호': 719,\n",
       " '칭찬': 720,\n",
       " '위': 721,\n",
       " '대해': 722,\n",
       " '종교': 723,\n",
       " '존중': 724,\n",
       " '바보': 725,\n",
       " '벌': 726,\n",
       " '소개': 727,\n",
       " '끊': 728,\n",
       " '드디어': 729,\n",
       " '똑같': 730,\n",
       " '멋진': 731,\n",
       " '탈': 732,\n",
       " '기간': 733,\n",
       " '이번': 734,\n",
       " '미리': 735,\n",
       " '그리고': 736,\n",
       " 'ㅎ': 737,\n",
       " '주일': 738,\n",
       " '대요': 739,\n",
       " '잖아요': 740,\n",
       " '쇼핑': 741,\n",
       " '예민': 742,\n",
       " '까먹': 743,\n",
       " '전환': 744,\n",
       " '월급': 745,\n",
       " '취직': 746,\n",
       " '써': 747,\n",
       " '여러': 748,\n",
       " '밖': 749,\n",
       " '지쳤': 750,\n",
       " '해볼까': 751,\n",
       " '은가': 752,\n",
       " '책': 753,\n",
       " '옆': 754,\n",
       " '한잔': 755,\n",
       " '인기': 756,\n",
       " '헷갈리': 757,\n",
       " '시기': 758,\n",
       " '통보': 759,\n",
       " '려': 760,\n",
       " '어느덧': 761,\n",
       " '부분': 762,\n",
       " '사친': 763,\n",
       " '로맨틱': 764,\n",
       " '거기': 765,\n",
       " '키우': 766,\n",
       " '개': 767,\n",
       " '슬프': 768,\n",
       " '어떻': 769,\n",
       " '어려워': 770,\n",
       " '알려줘': 771,\n",
       " '엔': 772,\n",
       " '뭔지': 773,\n",
       " '한다': 774,\n",
       " '커플': 775,\n",
       " '짓': 776,\n",
       " '꿨': 777,\n",
       " '심해': 778,\n",
       " '올려': 779,\n",
       " '맞춰': 780,\n",
       " '뭔가': 781,\n",
       " '분위기': 782,\n",
       " '걸로': 783,\n",
       " '인사': 784,\n",
       " '이사': 785,\n",
       " '피하': 786,\n",
       " '으면서': 787,\n",
       " '그대로': 788,\n",
       " '해결': 789,\n",
       " '자신감': 790,\n",
       " '타이밍': 791,\n",
       " '플': 792,\n",
       " '이성': 793,\n",
       " '어쩔': 794,\n",
       " '숨': 795,\n",
       " '그런가': 796,\n",
       " '흐르': 797,\n",
       " '어느': 798,\n",
       " '지켜보': 799,\n",
       " '더라고요': 800,\n",
       " '장난': 801,\n",
       " '해졌': 802,\n",
       " '당황': 803,\n",
       " '불편': 804,\n",
       " '치': 805,\n",
       " '군대': 806,\n",
       " '기다릴': 807,\n",
       " '기본': 808,\n",
       " '예쁜': 809,\n",
       " '멀': 810,\n",
       " '얼': 811,\n",
       " '이러': 812,\n",
       " '차리': 813,\n",
       " '자존': 814,\n",
       " '적응': 815,\n",
       " '거리': 816,\n",
       " '사과': 817,\n",
       " '선생': 818,\n",
       " '집중': 819,\n",
       " '돌아오': 820,\n",
       " '화장실': 821,\n",
       " '어려운': 822,\n",
       " '자유': 823,\n",
       " '최선': 824,\n",
       " '충전': 825,\n",
       " '서운': 826,\n",
       " '전해': 827,\n",
       " '경우': 828,\n",
       " '두려워': 829,\n",
       " '절대': 830,\n",
       " '한다면': 831,\n",
       " '아닐까요': 832,\n",
       " '버렸': 833,\n",
       " '샀': 834,\n",
       " '상': 835,\n",
       " '으니': 836,\n",
       " '몰랐': 837,\n",
       " '누가': 838,\n",
       " '자는': 839,\n",
       " '당당': 840,\n",
       " '비밀': 841,\n",
       " '원래': 842,\n",
       " '병원': 843,\n",
       " '위험': 844,\n",
       " '오랜만': 845,\n",
       " '돌아가': 846,\n",
       " '꾸준히': 847,\n",
       " '답장': 848,\n",
       " '적당히': 849,\n",
       " '청소': 850,\n",
       " '삭제': 851,\n",
       " '지우': 852,\n",
       " '못한': 853,\n",
       " '이기': 854,\n",
       " '조언': 855,\n",
       " '따로': 856,\n",
       " '썸녀': 857,\n",
       " '높': 858,\n",
       " '생기': 859,\n",
       " '현상': 860,\n",
       " '의사': 861,\n",
       " '더라': 862,\n",
       " '무서워': 863,\n",
       " '걷': 864,\n",
       " '오빠': 865,\n",
       " '꽃': 866,\n",
       " '데려다': 867,\n",
       " '발표': 868,\n",
       " '제대로': 869,\n",
       " '어색': 870,\n",
       " '복': 871,\n",
       " '잠수': 872,\n",
       " '뒤': 873,\n",
       " '일상': 874,\n",
       " '질투': 875,\n",
       " '코': 876,\n",
       " '학원': 877,\n",
       " '나을': 878,\n",
       " '려면': 879,\n",
       " '착각': 880,\n",
       " '완전히': 881,\n",
       " '경험': 882,\n",
       " '믿음': 883,\n",
       " '됩니다': 884,\n",
       " '본': 885,\n",
       " '매력': 886,\n",
       " '감사': 887,\n",
       " '했어요': 888,\n",
       " '적극': 889,\n",
       " '그건': 890,\n",
       " '됨': 891,\n",
       " '불': 892,\n",
       " '기운': 893,\n",
       " '그래': 894,\n",
       " '본인': 895,\n",
       " '고치': 896,\n",
       " '놈': 897,\n",
       " '휴가': 898,\n",
       " '의지': 899,\n",
       " '키': 900,\n",
       " '힘든가': 901,\n",
       " '떨려': 902,\n",
       " '크리스마스': 903,\n",
       " '짐': 904,\n",
       " '열': 905,\n",
       " '단': 906,\n",
       " '동생': 907,\n",
       " '피해': 908,\n",
       " '새벽': 909,\n",
       " '출근': 910,\n",
       " '언젠가': 911,\n",
       " '피부': 912,\n",
       " '점심': 913,\n",
       " '살짝': 914,\n",
       " '이거': 915,\n",
       " '당연': 916,\n",
       " '장': 917,\n",
       " '알려': 918,\n",
       " '귀엽': 919,\n",
       " '조절': 920,\n",
       " '식': 921,\n",
       " '10': 922,\n",
       " '장거리': 923,\n",
       " '사실': 924,\n",
       " '위한': 925,\n",
       " '놓아주': 926,\n",
       " '인가요': 927,\n",
       " '배려': 928,\n",
       " '할게요': 929,\n",
       " '지도': 930,\n",
       " '잊어버리': 931,\n",
       " '나눠': 932,\n",
       " '맘고생': 933,\n",
       " '헤아리': 934,\n",
       " '심하': 935,\n",
       " 'SNS': 936,\n",
       " '려나': 937,\n",
       " '막': 938,\n",
       " '다이어트': 939,\n",
       " '수록': 940,\n",
       " '어쩌': 941,\n",
       " '져': 942,\n",
       " '당했': 943,\n",
       " '그랬': 944,\n",
       " '줘도': 945,\n",
       " '담배': 946,\n",
       " '바뀌': 947,\n",
       " '좋아해': 948,\n",
       " '집안일': 949,\n",
       " '느껴': 950,\n",
       " '이름': 951,\n",
       " '들어가': 952,\n",
       " '늘': 953,\n",
       " '싸': 954,\n",
       " '빌려': 955,\n",
       " '맛': 956,\n",
       " '버스': 957,\n",
       " '면접': 958,\n",
       " '무섭': 959,\n",
       " '예뻐': 960,\n",
       " '자리': 961,\n",
       " '꺼': 962,\n",
       " '없애': 963,\n",
       " '연예인': 964,\n",
       " '아무리': 965,\n",
       " '안경': 966,\n",
       " '인상': 967,\n",
       " '알바': 968,\n",
       " '계획': 969,\n",
       " '명': 970,\n",
       " '7': 971,\n",
       " '보냈': 972,\n",
       " '견디': 973,\n",
       " '는다는': 974,\n",
       " '부탁': 975,\n",
       " '흔들리': 976,\n",
       " '반복': 977,\n",
       " '일지': 978,\n",
       " '그것': 979,\n",
       " '느끼': 980,\n",
       " '이용': 981,\n",
       " '눈치': 982,\n",
       " '발전': 983,\n",
       " '으시': 984,\n",
       " '떨어졌': 985,\n",
       " '휴식': 986,\n",
       " '살까': 987,\n",
       " '피': 988,\n",
       " '졸려': 989,\n",
       " '신': 990,\n",
       " '연락처': 991,\n",
       " '졸업': 992,\n",
       " '존재': 993,\n",
       " '냄새': 994,\n",
       " '줘야': 995,\n",
       " '의심': 996,\n",
       " '배': 997,\n",
       " '노': 998,\n",
       " '도서관': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = keras.preprocessing.sequence.pad_sequences(que_train, padding='pre', maxlen=20)\n",
    "dec_train = keras.preprocessing.sequence.pad_sequences(ans_train, padding='pre', maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0, 2322,  175, 4672,   85], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         2, 250,   9, 143,   9,  35,   3], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=5,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc802232dc59456cac7c106752ee9dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb03e3626b8483a9ccf2f9a8659b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342d4d5f6a8f440c9f9c94834bb4b8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c7ddbbd5947a28fcf159ff4852852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bef481fba349b68a51e77481d055e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1634430905448cf97b151c9431bf00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c57e43dc745cfbc80516065321d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3997f600146249f0b6fa4364b283dcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd169637a8c47f6b7b48132081dda07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c5c79079fa4869b315d55c1c68a577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e1cf201eee45de9914557d844b76bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1434d17e7348d6a5bca06180b91069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fc07b1912b4368830d1ca033b44e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efb9a92e1d14a57b557798df3b87ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f56b50b0084657a6225d10e8123abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5341dfb6cb4d40498c8e60f63ad0f8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f50e261ce04376938dcb8e1212468d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd60b4b24054c7f902c20d98e97b830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab7dee497ed4aa3a2f8968c55892abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1459b6b37e4d5da6618b9f7860d091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121224902c5544de800e10341a68842d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9ea93e65b64d4d96a0f89b7dc0c50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f501731c6463406fa71cbe469aa6d0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2670f6d177949159d613569b5925adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c5cd17093d4f319f538c921c807735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e8109dda0144eea60e5ac5d5975d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65c3cbef7874fa493aa441ecbfcc8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6164a95537f34b139634b322c291078b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e1f20f491946da80af97a6ec4d8f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa20cee4769a41e8906654ab47076412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = mecab.morphs(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in pieces:\n",
    "        sen= get_encoded_sentence(sen, word_to_index)\n",
    "        tokens.append(sen)\n",
    "    \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                        value=word_to_index[\"<pad>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=20)\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"지루하다, 놀러가고 싶어.\", \"오늘 일찍 일어났더니 피곤하다.\", \"간만에 여자친구랑 데이트 하기로 했어.\", \"집에 있는다는 소리야.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample :  지루하다, 놀러가고 싶어.\n",
      "Translations :  이 있 으면 인기 있 는 거 라면 안 줘도 정리 하 는 게 좋 지 죠\n",
      "sample :  오늘 일찍 일어났더니 피곤하다.\n",
      "Translations :  은 더 좋 아 하 는 것 들 아 할 때 있 는 것 만 있 어요\n",
      "sample :  간만에 여자친구랑 데이트 하기로 했어.\n",
      "Translations :  을 하 면 이야기 하 지 말 고 생각 하 면 기분 이 없 을 거 예요\n",
      "sample :  집에 있는다는 소리야.\n",
      "Translations :  오 고 집 이 라면 후회 안 한 집 마련 이 라면 풀렸 고 오 고 오 세요\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    print('sample : ', sample)\n",
    "    print('Translations : ', translate(sample, transformer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 회고록  \n",
    "트랜스포머가 제대로 이해가 안되서 그런지 노드를 진행하는게 매우 어려웠다.  \n",
    "특히 이전에는 단순히 코드를 이해하는 것에 그쳤다면 고잉디퍼에서는 하나하나 새로 만들어야되는 부분이 어려운 부분중에 하나인 것 같다.  \n",
    "이번에도 전반부만 하고 마무리를 못지을뻔 했는데 다행히 구글링을 통해 오류를 해결할 수 있었다.  \n",
    "앞으로 남은 고잉디퍼노드들이 걱정되는 부분이긴 하다.  \n",
    "그래도 일단 열심히 진행해보려고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
