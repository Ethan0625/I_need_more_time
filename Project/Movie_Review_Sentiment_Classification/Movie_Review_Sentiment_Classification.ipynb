{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie_Review_Sentiment_Classification\n",
    "### (영화리뷰 감성분석)\n",
    "\n",
    "***\n",
    "#### 1. 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "#### 2. 데이터로더 구성\n",
    "> **1) 데이터의 중복 제거  \n",
    "2) NaN 결측치 제거  \n",
    "3) 한국어 토크나이저로 토큰화  \n",
    "4) 불용어(Stopwords) 제거  \n",
    "5) 사전word_to_index 구성  \n",
    "6) 텍스트 스트링을 사전 인덱스 스트링으로 변환  \n",
    "7) X_train, y_train, X_test, y_test, word_to_index 리턴**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터의 중복단어 갯수 확인\n",
    "# .nunique()는 판다스에서 유니크한 값의 갯수를 알려줌\n",
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49157, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 150000 => 146182로 중복 확인\n",
    "\n",
    "# 테스트데이터의 중복단어 여부 확인\n",
    "test_data['document'].nunique(), test_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 정리 한 학습용 리뷰 개수 : 146182\n"
     ]
    }
   ],
   "source": [
    "# 50000 => 49157로 중복 확인\n",
    "\n",
    "# 데이터의 중복 제거 및 이상한 데이터 삭제\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data = train_data.dropna(how='any')\n",
    "print('데이터 정리 한 학습용 리뷰 개수 :',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    # [[YOUR CODE]]\n",
    "    # 1-1) 학습데이터의 중복 제거\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "#     1-2) 학습데이터에서 NaN 결측치 제거\n",
    "    train_data =train_data.dropna(how='any')\n",
    "    \n",
    "#     2-1) 테스트데이터의 중복 제거\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "#     2-2) 테스트데이터의 NaN 결측치 제거\n",
    "    test_data =train_data.dropna(how='any')\n",
    "    \n",
    "#     3-1) 학습데이터 한국어 토크나이저로 토큰화\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        # sentence에 있는 문장을 뜻을 가질수 있는 최소단위 '형태소'로 \n",
    "        # 분리해서 토큰으로 가지고 있어줘\n",
    "        temp_X = tokenizer.morphs(sentence) \n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어(stopwords) 제거\n",
    "        # temp_X의 토큰을 X_train에 넣어줘.\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "#     3-2) 테스트데이터 한국어 토크나이저로 토큰화\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence)\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어(stopwords) 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "#     5) 사전word_to_index 구성\n",
    "    # np.concatenate(배열) : 해당 배열을 오른쪽에 추가하기\n",
    "    # .tolost() : 앞의 배열을 np.unit이 아닌 list로 바꿔줌\n",
    "    # => words는 X_train의 배열을 가진 list가 됨.\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    # Counter(words)\n",
    "    # words에서 단어들의 갯수를 세어 {단어1:갯수, 단어2:갯수, ...} 형태의\n",
    "    # 딕셔너리를 형성함.\n",
    "    # ex) words = [가, 나, 가, 다, 나, 가, 라] 일때\n",
    "    # Counter(words)를 하면\n",
    "    # 결과물로 ({가:3, 나:2, 다:1, 라:1})의 형태임.\n",
    "    counter = Counter(words)\n",
    "    # counter.most_common(10000-4)\n",
    "    # conter에서 가장 많이 사용된 단어를 [(단어1, 횟수),(단어2, 횟수), ...]의 형태로\n",
    "    # 정렬하여 가장 많이 나온 횟수부터 가장 적게나온 횟수 순으로 (10000-4)개를 정렬해준다.\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "#     6) 텍스트 스트링을 사전 인덱스 스트링으로 변환  \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    # map() 함수는 for문과 같은 반복문을 사용하지 않아도 \n",
    "    # 지정한 함수로 인자를 여러번 전달해 그 결과를 list 형태로 \n",
    "    # 뽑아 주는 유용한 함수이다.\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "#     7) X_train, y_train, X_test, y_test, word_to_index 리턴\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 리뷰 개수 : 146183\n",
      "테스트용 리뷰 개수 : 49158\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 갯수 확인\n",
    "print('학습용 리뷰 개수 :',len(train_data))\n",
    "print('테스트용 리뷰 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
